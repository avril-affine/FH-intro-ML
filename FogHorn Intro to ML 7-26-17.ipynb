{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "### What is Machine Learning?\n",
    "\n",
    "Machine learning explores the study and construction of algorithms that can learn from and make predictions on **data**. Data allows us to evaluate how good or bad the algorithm is doing. Based on the evaluation, we can update the algorithm to perform \"better\".\n",
    "\n",
    "There are two main categories of machine learning algorithms: supervised and unsupervised learning. \n",
    "\n",
    "Supervised learning is the task of constructing an algorithm to perform predictions with labeled data. For example, if we are trying to predict windmill power generated we may want to gather data on temperature, wind speed, time of day (features) and the power generated (labels).\n",
    "\n",
    "Unsupervised learning learning is the task of constructing an algorithm without labeled data.\n",
    "\n",
    "### Examples\n",
    "- Windmill power forecasting\n",
    "- Stock price prediction\n",
    "- Predicting capacitor failure\n",
    "- Image classification\n",
    "- News article clustering\n",
    "\n",
    "### Data\n",
    "\n",
    "The data used will likely be able to be structured in a csv (excel spreadsheet). Data is usually represented as one row per observation. Each column represents a feature of that observation.\n",
    "\n",
    "Example: Dataset from kaggle \n",
    "\n",
    "<img src=\"images/csv_image.png\" />\n",
    "[Source](http://blog.trifork.com/2017/02/16/machine-learning-predicting-house-prices/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 1   # Will not plot if this variable is changed\n",
    "noise = 10.\n",
    "# bias = np.random.rand() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_regression(n_features=n_features, noise=noise) # bias=bias)\n",
    "N = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "We can choose our algorithm to be a linear model. Our algorithm will have the form:\n",
    "\n",
    "$$\\hat{y} = WX + b$$\n",
    "\n",
    "where $\\hat{y}$ is our prediction for $y$, $X$ is the features, $W$ is the weights for each feature, and $b$ is the intercept. Linear regression can be used to model data with labels that are continuous.\n",
    "\n",
    "*Note: uppercase variables are matrices and lowercase variables are vectors.*\n",
    "\n",
    "### How do we find the right W?\n",
    "\n",
    "We have already chosen our algorithm, now we have to define how well that algorithm does. This will be called our cost function:\n",
    "\n",
    "$$ C = \\frac{1}{2N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "where $y$ is the true value, $\\hat{y}$ is our prediction for $y$, and $N$ is the number of observations. $C$ is our cost function which will evaluate how good our model is (the lower the better).\n",
    "\n",
    "Since this example uses a single variable we can easily visualize the cost for all $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(X, W, b=None):\n",
    "    if b:\n",
    "        y_hat = X.dot(W.T) + b\n",
    "    else:\n",
    "        y_hat = X.dot(W.T)\n",
    "    return y_hat.reshape(-1)\n",
    "\n",
    "def cost(X, W, y, b=None):\n",
    "    N = X.shape[0]\n",
    "    y_hat = f(X, W, b)\n",
    "    C = (1./ (2. * N)) * np.sum((y - y_hat) ** 2)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ws = np.linspace(30, 55)\n",
    "Cs = []\n",
    "for w in Ws:\n",
    "    w = np.array(w)\n",
    "    Cs.append(cost(X, w, y))\n",
    "plt.scatter(Ws, Cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "The set of $W$ and $b$s that we want is the one that gives us the lowest cost. With some calculus we can calculate the derivate of $C$ with respect to $W$. Using the derivative, we can find the optimal $W$. (Remember the chain rule).\n",
    "\n",
    "$$ \\frac{dC}{d\\hat{y}} = \\frac{1}{N}(y - \\hat{y}) $$\n",
    "$$ \\frac{d\\hat{y}}{dW} = X $$\n",
    "$$ \\frac{dC}{dW} = \\frac{dC}{d\\hat{y}}\\frac{d\\hat{y}}{dW} = (y - \\hat{y}) \\cdot X $$\n",
    "\n",
    "We can do a similar calculation for $b$, except $\\frac{d\\hat{y}}{db} = 1$.\n",
    "\n",
    "We will use the direction of the derivative to find the lowest point of the function, which represents the minimum of the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad(X, W, y, b=None):\n",
    "    N = X.shape[0]\n",
    "    y_hat = f(X, W, b)\n",
    "    diff = (y_hat - y)\n",
    "    gradW = diff.dot(X) / N\n",
    "    gradb = np.sum(diff) / N\n",
    "    return gradW, gradb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with the initial choice for $W$ and $b$ to be all zeros. We will then use gradient descent to find the optimal $W$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.25\n",
    "\n",
    "W = np.zeros((1, n_features))\n",
    "b = 0\n",
    "\n",
    "Xs = np.linspace(-3, 3)    # for plotting\n",
    "costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute this cell to iterate and visualize the updated weights\n",
    "\n",
    "gradW, gradb = grad(X, W, y, b)\n",
    "W -= learning_rate * gradW\n",
    "b -= learning_rate * gradb\n",
    "costs.append(cost(X, W, y, b))\n",
    "\n",
    "y_hat = f(Xs.reshape(-1, 1), W, b)\n",
    "plt.scatter(X, y)\n",
    "plt.plot(Xs, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize how the cost goes down for every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(costs)), costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is used for classification problems. Typically, the labels are binary and can be represented as 1/0 (pass/fail). We will need a new algorithm to model our data. We will use the sigmoid function. This function squashes the output to between 0 and 1, like a probability.\n",
    "\n",
    "$$ \\sigma (z) = \\hat{y} = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "We will refer to the sigmoid function as $\\sigma (z)$, where $z = WX + b$.\n",
    "\n",
    "The function looks like this.\n",
    "\n",
    "<img src=\"images/sigmoid.png\" />\n",
    "\n",
    "We will also need to modify our cost function. We will want to heavily penalize predictions close to 0 when the true value is close to 1 and vice-versa. To do this we can use the log-loss function.\n",
    "\n",
    "$$ C = -\\frac{1}{N} \\sum_{i=1}^{N} y_i log(\\hat{y}_i) + (1 - y_i) log(1 - \\hat{y}_i) $$\n",
    "\n",
    "The first term in the summation is multiplied by the true value, $y$. Notice when this is 0, this term will also be 0. Also, the second term will be 0, when the $y$ is 1. This leaves us with the log($\\hat{y}$) which is large when $\\hat{y}$ is close to 0 and small when $\\hat{y}$ is close to 1. This cost functions characterizes how we will penalize strong predictions of the opposite class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 1e-7    # use so we don't get log(0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    y_hat = 1. / (1 + np.exp(-z))\n",
    "    return y_hat\n",
    "\n",
    "def log_loss(X, W, y, b=None):\n",
    "    N = X.shape[0]\n",
    "    z = f(X, W, b)\n",
    "    y_hat = sigmoid(z)\n",
    "    y_hat = np.clip(y_hat, eps, 1 - eps)\n",
    "    C = -(1./N) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time let's use 2 features since we can use a color to visualize the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_colors(y_):\n",
    "    return ['b' if p == 1 else 'r' for p in y_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_features=n_features, n_informative=n_features, n_redundant=0, shift=5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], color=get_colors(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent (part 2)\n",
    "\n",
    "Again, we will use gradient descent to calculate the optimal $W$ and $b$. The gradient has also changed since we updated our cost function. Doing some calculus we get the derivate as:\n",
    "\n",
    "$$ \\frac{dlog(C)}{d\\hat{y}} = \\hat{y} - y $$\n",
    "$$ \\frac{d\\hat{y}}{dW} = -\\sigma (z) (1 - \\sigma (z)) \\cdot X $$\n",
    "\n",
    "Refer to [here](https://en.wikipedia.org/wiki/Logistic_function) for more math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_log(X, W, y, b=None):\n",
    "    N = X.shape[0]\n",
    "    z = f(W, X, b)\n",
    "    y_hat = sigmoid(z)\n",
    "    gradW = y_hat - y\n",
    "    gradW *= y_hat * (1 - y_hat)\n",
    "    gradW = gradW.reshape(1, -1).dot(X)\n",
    "    gradb = y_hat - y\n",
    "    gradb *= y_hat * (1 - y_hat)\n",
    "    gradb = np.sum(gradb)\n",
    "    return gradW, gradb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "\n",
    "W = np.zeros((1, n_features))\n",
    "b = 0\n",
    "\n",
    "costs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradW, gradb = grad_log(X, W, y, b)\n",
    "W -= learning_rate * gradW\n",
    "b -= learning_rate * gradb\n",
    "costs.append(log_loss(X, W, y, b))\n",
    "\n",
    "z = f(X, W, b)\n",
    "y_hat = sigmoid(z)\n",
    "y_hat = y_hat > 0.5    # Make binary predictions (0/1)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], color=get_colors(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the cost for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(costs)), costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
